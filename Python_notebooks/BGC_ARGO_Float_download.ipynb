{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish necessary modules\n",
    "\n",
    "import paramiko\n",
    "import functools\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from urllib.request import urlretrieve\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish necessary directories\n",
    "\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "data_dir   = os.path.join(parent_dir, 'Data')\n",
    "navis_dir  = os.path.join(data_dir, 'navis')\n",
    "navis_101  = os.path.join(navis_dir, 'navis_101')\n",
    "navis_102  = os.path.join(navis_dir, 'navis_102')\n",
    "n101_folder = os.path.join(navis_101, 'csv_files')\n",
    "n102_folder = os.path.join(navis_102, 'csv_files')\n",
    "\n",
    "# If directories don't exist, create them\n",
    "if not os.path.exists(navis_dir):\n",
    "    os.makedirs(navis_dir)\n",
    "if not os.path.exists(navis_101):\n",
    "    os.makedirs(navis_101)\n",
    "if not os.path.exists(navis_102):\n",
    "    os.makedirs(navis_102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navis float download section\n",
    "\n",
    "# Open a transport\n",
    "host,port = \"rudics.thorium.cls.fr\",22\n",
    "transport = paramiko.Transport((host,port))\n",
    "\n",
    "# Auth\n",
    "username_list = ['s_f1101','s_f1102']\n",
    "password_list = [\"KhuBGFTj\",\"hckrJHGY\"]\n",
    "directory_list = [navis_101, navis_102]\n",
    "\n",
    "#username,password = \"s_f1102\",\"hckrJHGY\"\n",
    "#username,password = \"s_f1101\",\"KhuBGFTj\"\n",
    "\n",
    "for username, password, out_dir in zip(username_list, password_list, directory_list):\n",
    "    \n",
    "    transport = paramiko.Transport((host,port))\n",
    "    transport.connect(None,username,password)\n",
    "\n",
    "    # Create an SFTP session\n",
    "    sftp = paramiko.SFTPClient.from_transport(transport)\n",
    "    print(\"Session opened\")\n",
    "\n",
    "    # List files in the root directory or any specific directory\n",
    "    directory = '/'  # Change this to your target directory\n",
    "    files = sftp.listdir(directory)\n",
    "\n",
    "    # Print the list of files\n",
    "    print(f\"Files in {directory}:\")\n",
    "    for file in files:\n",
    "        print(file)\n",
    "\n",
    "    msg_files = [file for file in files if file.endswith('.msg')]\n",
    "    # Download each .msg file\n",
    "    for msg_file in tqdm(msg_files):\n",
    "        local_path = os.path.join(out_dir, msg_file)\n",
    "        remote_path = os.path.join(directory, msg_file)\n",
    "\n",
    "        sftp.get(remote_path, local_path)\n",
    "\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "    sftp.close()\n",
    "\n",
    "    print(\"Session closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hanshil\\AppData\\Local\\Temp\\ipykernel_22340\\3025249952.py:32: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  last_position_df = pd.concat([last_position_df, last_temp_df], ignore_index=True)\n",
      "C:\\Users\\hanshil\\AppData\\Local\\Temp\\ipykernel_22340\\3025249952.py:33: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  position_df = pd.concat([position_df, temp_df], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Other, WMO, floats\n",
    "\n",
    "wmo_list = [4903532, 1902637]\n",
    "#Float 1 = test float in the Icelandic Bassin\n",
    "float_1_url = 'https://data-argo.ifremer.fr/dac/aoml/4903532/4903532_Sprof.nc'\n",
    "#Float 2 = test float on Custard with glider next to it\n",
    "float_2_url = 'https://data-argo.ifremer.fr/dac/coriolis/1902637/1902637_Sprof.nc'\n",
    "\n",
    "#List the floats\n",
    "floats_url = [float_1_url, float_2_url]\n",
    "\n",
    "#Assign the local float directory\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "floats_dir =  os.path.join(parent_dir, 'Data\\\\Floats')\n",
    "\n",
    "#Create floats filename\n",
    "floats_filenames = []\n",
    "for i in floats_url:\n",
    "    filename = floats_dir + '/' + i.rsplit('/', 1)[1]\n",
    "    floats_filenames.append(filename)\n",
    "\n",
    "position_df = pd.DataFrame({'PROF_NUM' : str(), 'LONGITUDE' : [], 'LATITUDE' : [], 'float' : int()})\n",
    "last_position_df = pd.DataFrame({'PROF_NUM' : str(), 'LONGITUDE' : [], 'LATITUDE' : [], 'float' : int()})\n",
    "\n",
    "for file, wmo in zip(floats_filenames, wmo_list):\n",
    "    dat = xr.open_dataset(file)\n",
    "    dat = dat.rename({'CYCLE_NUMBER':'PROF_NUM'}).swap_dims({'N_PROF':'PROF_NUM'})\n",
    "    temp_df = dat[['LONGITUDE', 'LATITUDE', 'JULD']].to_dataframe().reset_index()\n",
    "    temp_df['float'] = wmo\n",
    "    last_temp_df = temp_df[temp_df['JULD'] == max(temp_df['JULD'])]\n",
    "\n",
    "    last_position_df = pd.concat([last_position_df, last_temp_df], ignore_index=True)\n",
    "    position_df = pd.concat([position_df, temp_df], ignore_index=True)\n",
    "    dat.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/88 [00:00<?, ?it/s]C:\\Users\\hanshil\\AppData\\Local\\Temp\\ipykernel_22340\\1523172333.py:48: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  compiled_101 = pd.concat([compiled_101, temp_df], ignore_index= True)\n",
      "100%|██████████| 88/88 [00:00<00:00, 108.33it/s]\n",
      "  0%|          | 0/93 [00:00<?, ?it/s]C:\\Users\\hanshil\\AppData\\Local\\Temp\\ipykernel_22340\\1523172333.py:65: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  compiled_102 = pd.concat([compiled_102, temp_df], ignore_index= True)\n",
      "100%|██████████| 93/93 [00:00<00:00, 113.26it/s]\n"
     ]
    }
   ],
   "source": [
    "# Processing all float data into single dataframe\n",
    "\n",
    "def open_navis(filepath, float_ref):\n",
    "    raw_var = ['mtime', 'pnum', 'lat', 'lon', 'pres', 'T', 'C', 'oxy1', 'oxy2', 'mcoms1', 'mcoms2', 'mcoms3']\n",
    "    new_colnames = ['datetime', 'prof', 'lat', 'lon', 'pres', 'temp', 'conductivity', 'oxy1', 'oxy2', 'mcoms1', 'mcoms2', 'mcoms3']\n",
    "    df = pd.read_csv(filepath)\n",
    "    df_raw = df[raw_var]\n",
    "    df_raw.columns = new_colnames\n",
    "\n",
    "    df_computed = df_raw.copy()\n",
    "\n",
    "    df_computed.loc[:, 'Fchl'] = (df_raw['mcoms1'] - chl_dark) * chl_slope\n",
    "    df_computed.loc[:, 'beta'] = (df_raw['mcoms2'] - beta_dark) * beta_slope\n",
    "    df_computed.loc[:, 'fdom'] = (df_raw['mcoms3'] - fdom_dark) * fdom_slope\n",
    "\n",
    "    df_computed.loc[:, 'float'] = float_ref\n",
    "\n",
    "    return(df_computed)\n",
    "\n",
    "def clean_datetime_column(df, column):\n",
    "    # Identify rows that do not match the expected datetime format\n",
    "    mask = df[column].str.contains(r'^\\d{4}/\\d{2}/\\d{2} \\d{2}:\\d{2}:\\d{2}$', regex=True)\n",
    "    \n",
    "    # Print the rows that are problematic\n",
    "    print(\"\\nProblematic entries:\")\n",
    "    print(df[~mask])\n",
    "    \n",
    "    # Filter out problematic rows or handle them accordingly\n",
    "    cleaned_df = df[mask].copy()\n",
    "    \n",
    "    return cleaned_df\n",
    "\n",
    "#set the cal val for N101\n",
    "chl_dark = 51\n",
    "chl_slope = 0.001553\n",
    "\n",
    "beta_dark = 92\n",
    "beta_slope = 0.0000002485\n",
    "\n",
    "fdom_dark = 52\n",
    "fdom_slope = 0.01118\n",
    "\n",
    "new_colnames = ['datetime', 'prof', 'lat', 'lon', 'pres', 'temp', 'conductivity', 'oxy1', 'oxy2', 'mcoms1', 'mcoms2', 'mcoms3']\n",
    "compiled_101 = pd.DataFrame(columns=new_colnames)\n",
    "for file in tqdm(os.listdir(n101_folder)):\n",
    "    filepath = os.path.join(n101_folder, file)\n",
    "    temp_df = open_navis(filepath, 'navis101')\n",
    "    compiled_101 = pd.concat([compiled_101, temp_df], ignore_index= True)\n",
    "\n",
    "#set the cal val for N102\n",
    "chl_dark = 50\n",
    "chl_slope = 0.002006\n",
    "\n",
    "beta_dark = 49\n",
    "beta_slope = 0.0000003524\n",
    "\n",
    "fdom_dark = 51\n",
    "fdom_slope = 0.006619\n",
    "\n",
    "new_colnames = ['datetime', 'prof', 'lat', 'lon', 'pres', 'temp', 'conductivity', 'oxy1', 'oxy2', 'mcoms1', 'mcoms2', 'mcoms3']\n",
    "compiled_102 = pd.DataFrame(columns=new_colnames)\n",
    "for file in tqdm(os.listdir(n102_folder)):\n",
    "    filepath = os.path.join(n102_folder, file)\n",
    "    temp_df = open_navis(filepath, 'navis102')\n",
    "    compiled_102 = pd.concat([compiled_102, temp_df], ignore_index= True)\n",
    "\n",
    "navis_df = pd.concat([compiled_101, compiled_102])\n",
    "\n",
    "navis_position = navis_df[['datetime', 'prof', 'float', 'lon', 'lat']].drop_duplicates(subset = ['prof', 'float'])\n",
    "navis_df['JULD'] = pd.to_datetime(navis_df['datetime'])\n",
    "navis_df.to_csv(os.path.join(navis_dir, 'merged_table'))\n",
    "\n",
    "navis_position.columns = ['JULD', 'PROF_NUM', 'float', 'LONGITUDE', 'LATITUDE']\n",
    "navis_position['JULD'] = navis_position['JULD'].astype(str)\n",
    "\n",
    "cleaned_navis_position = navis_position.copy()\n",
    "cleaned_navis_position.head()\n",
    "cleaned_navis_position['JULD'] = pd.to_datetime(cleaned_navis_position['JULD'])\n",
    "\n",
    "navis_position['JULD'] = pd.to_datetime(navis_position['JULD'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output with all float positions, up to date.\n",
    "\n",
    "full_position = pd.concat([position_df, navis_position], ignore_index = True)\n",
    "full_position.to_csv(os.path.join(floats_dir, 'Float_positions.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BIOCarbon_Conda_Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
