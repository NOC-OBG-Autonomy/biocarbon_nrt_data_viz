{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import xarray as xr\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir    = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "data_dir      = os.path.join(parent_dir, 'data')\n",
    "navis_dir = os.path.join(data_dir, 'navis')\n",
    "n101_folder = os.path.join(navis_dir, 'navis_101/csv_files')\n",
    "n102_folder = os.path.join(navis_dir, 'navis_102/csv_files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the cal val for N101\n",
    "chl_dark = 51\n",
    "chl_slope = 0.001553\n",
    "\n",
    "beta_dark = 92\n",
    "beta_slope = 0.0000002485\n",
    "\n",
    "fdom_dark = 52\n",
    "fdom_slope = 0.01118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_navis(filepath, float_ref):\n",
    "    raw_var = ['mtime', 'pnum', 'lat', 'lon', 'pres', 'T', 'C', 'oxy1', 'oxy2', 'mcoms1', 'mcoms2', 'mcoms3']\n",
    "    new_colnames = ['datetime', 'prof', 'lat', 'lon', 'pres', 'temp', 'conductivity', 'oxy1', 'oxy2', 'mcoms1', 'mcoms2', 'mcoms3']\n",
    "    df = pd.read_csv(filepath)\n",
    "    df_raw = df[raw_var]\n",
    "    df_raw.columns = new_colnames\n",
    "\n",
    "    df_computed = df_raw.copy()\n",
    "\n",
    "    df_computed.loc[:, 'Fchl'] = (df_raw['mcoms1'] - chl_dark) * chl_slope\n",
    "    df_computed.loc[:, 'beta'] = (df_raw['mcoms2'] - chl_dark) * chl_slope\n",
    "    df_computed.loc[:, 'fdom'] = (df_raw['mcoms3'] - chl_dark) * chl_slope\n",
    "\n",
    "    df_computed.loc[:, 'float'] = float_ref\n",
    "\n",
    "    return(df_computed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/88 [00:00<?, ?it/s]C:\\Users\\hanshil\\AppData\\Local\\Temp\\ipykernel_2528\\213775207.py:6: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  compiled_101 = pd.concat([compiled_101, temp_df], ignore_index= True)\n",
      "100%|██████████| 88/88 [00:00<00:00, 88.46it/s] \n"
     ]
    }
   ],
   "source": [
    "new_colnames = ['datetime', 'prof', 'lat', 'lon', 'pres', 'temp', 'conductivity', 'oxy1', 'oxy2', 'mcoms1', 'mcoms2', 'mcoms3']\n",
    "compiled_101 = pd.DataFrame(columns=new_colnames)\n",
    "for file in tqdm(os.listdir(n101_folder)):\n",
    "    filepath = os.path.join(n101_folder, file)\n",
    "    temp_df = open_navis(filepath, 'navis101')\n",
    "    compiled_101 = pd.concat([compiled_101, temp_df], ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the cal val for N102\n",
    "chl_dark = 50\n",
    "chl_slope = 0.002006\n",
    "\n",
    "beta_dark = 49\n",
    "beta_slope = 0.0000003524\n",
    "\n",
    "fdom_dark = 51\n",
    "fdom_slope = 0.006619"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/93 [00:00<?, ?it/s]C:\\Users\\hanshil\\AppData\\Local\\Temp\\ipykernel_2528\\798626295.py:6: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  compiled_102 = pd.concat([compiled_102, temp_df], ignore_index= True)\n",
      "100%|██████████| 93/93 [00:00<00:00, 96.16it/s] \n"
     ]
    }
   ],
   "source": [
    "new_colnames = ['datetime', 'prof', 'lat', 'lon', 'pres', 'temp', 'conductivity', 'oxy1', 'oxy2', 'mcoms1', 'mcoms2', 'mcoms3']\n",
    "compiled_102 = pd.DataFrame(columns=new_colnames)\n",
    "for file in tqdm(os.listdir(n102_folder)):\n",
    "    filepath = os.path.join(n102_folder, file)\n",
    "    temp_df = open_navis(filepath, 'navis102')\n",
    "    compiled_102 = pd.concat([compiled_102, temp_df], ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "navis_df = pd.concat([compiled_101, compiled_102])\n",
    "navis_position = navis_df[['datetime', 'prof', 'float', 'lon', 'lat']].drop_duplicates(subset = ['prof', 'float'])\n",
    "navis_df['JULD'] = pd.to_datetime(navis_df['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "navis_df.to_csv(os.path.join(navis_dir, 'merged_table'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "navis_position.columns = ['JULD', 'PROF_NUM', 'float', 'LONGITUDE', 'LATITUDE']\n",
    "navis_position['JULD'] = navis_position['JULD'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_datetime_column(df, column):\n",
    "    # Identify rows that do not match the expected datetime format\n",
    "    mask = df[column].str.contains(r'^\\d{4}/\\d{2}/\\d{2} \\d{2}:\\d{2}:\\d{2}$', regex=True)\n",
    "    \n",
    "    # Print the rows that are problematic\n",
    "    print(\"\\nProblematic entries:\")\n",
    "    print(df[~mask])\n",
    "    \n",
    "    # Filter out problematic rows or handle them accordingly\n",
    "    cleaned_df = df[mask].copy()\n",
    "    \n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_navis_position = navis_position.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JULD</th>\n",
       "      <th>PROF_NUM</th>\n",
       "      <th>float</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>LATITUDE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022/07/25 20:38:11</td>\n",
       "      <td>2</td>\n",
       "      <td>navis101</td>\n",
       "      <td>-23.1665</td>\n",
       "      <td>58.1415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>2022/07/30 21:10:54</td>\n",
       "      <td>3</td>\n",
       "      <td>navis101</td>\n",
       "      <td>-22.6485</td>\n",
       "      <td>58.4391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>2022/08/04 13:19:01</td>\n",
       "      <td>4</td>\n",
       "      <td>navis101</td>\n",
       "      <td>-21.9113</td>\n",
       "      <td>58.7442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1465</th>\n",
       "      <td>2022/08/09 01:28:42</td>\n",
       "      <td>5</td>\n",
       "      <td>navis101</td>\n",
       "      <td>-22.5342</td>\n",
       "      <td>59.1069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1955</th>\n",
       "      <td>2022/08/13 12:31:46</td>\n",
       "      <td>6</td>\n",
       "      <td>navis101</td>\n",
       "      <td>-22.5308</td>\n",
       "      <td>59.3274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     JULD PROF_NUM     float  LONGITUDE  LATITUDE\n",
       "0     2022/07/25 20:38:11        2  navis101   -23.1665   58.1415\n",
       "489   2022/07/30 21:10:54        3  navis101   -22.6485   58.4391\n",
       "977   2022/08/04 13:19:01        4  navis101   -21.9113   58.7442\n",
       "1465  2022/08/09 01:28:42        5  navis101   -22.5342   59.1069\n",
       "1955  2022/08/13 12:31:46        6  navis101   -22.5308   59.3274"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_navis_position.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_navis_position['JULD'] = pd.to_datetime(cleaned_navis_position['JULD'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hanshil\\AppData\\Local\\Temp\\ipykernel_2528\\173837214.py:30: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  last_position_df = pd.concat([last_position_df, last_temp_df], ignore_index=True)\n",
      "C:\\Users\\hanshil\\AppData\\Local\\Temp\\ipykernel_2528\\173837214.py:31: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  position_df = pd.concat([position_df, temp_df], ignore_index=True)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "did not find a match in any of xarray's currently installed IO backends ['netcdf4', 'h5netcdf', 'scipy', 'zarr']. Consider explicitly selecting one of the installed engines via the ``engine`` parameter, or installing additional IO dependencies, see:\nhttps://docs.xarray.dev/en/stable/getting-started-guide/installing.html\nhttps://docs.xarray.dev/en/stable/user-guide/io.html",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m last_position_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPROF_NUM\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28mstr\u001b[39m(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLONGITUDE\u001b[39m\u001b[38;5;124m'\u001b[39m : [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLATITUDE\u001b[39m\u001b[38;5;124m'\u001b[39m : [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28mint\u001b[39m()})\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file, wmo \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(floats_filenames, wmo_list):\n\u001b[1;32m---> 24\u001b[0m     dat \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     dat \u001b[38;5;241m=\u001b[39m dat\u001b[38;5;241m.\u001b[39mrename({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCYCLE_NUMBER\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPROF_NUM\u001b[39m\u001b[38;5;124m'\u001b[39m})\u001b[38;5;241m.\u001b[39mswap_dims({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN_PROF\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPROF_NUM\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m     26\u001b[0m     temp_df \u001b[38;5;241m=\u001b[39m dat[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLONGITUDE\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLATITUDE\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJULD\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mto_dataframe()\u001b[38;5;241m.\u001b[39mreset_index()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\xarray\\backends\\api.py:554\u001b[0m, in \u001b[0;36mopen_dataset\u001b[1;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m    551\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate(backend_kwargs)\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 554\u001b[0m     engine \u001b[38;5;241m=\u001b[39m \u001b[43mplugins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mguess_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_array_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    557\u001b[0m     from_array_kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\xarray\\backends\\plugins.py:197\u001b[0m, in \u001b[0;36mguess_engine\u001b[1;34m(store_spec)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    191\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound the following matches with the input file in xarray\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms IO \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    192\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackends: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompatible_engines\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. But their dependencies may not be installed, see:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    193\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.xarray.dev/en/stable/user-guide/io.html \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.xarray.dev/en/stable/getting-started-guide/installing.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    195\u001b[0m     )\n\u001b[1;32m--> 197\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg)\n",
      "\u001b[1;31mValueError\u001b[0m: did not find a match in any of xarray's currently installed IO backends ['netcdf4', 'h5netcdf', 'scipy', 'zarr']. Consider explicitly selecting one of the installed engines via the ``engine`` parameter, or installing additional IO dependencies, see:\nhttps://docs.xarray.dev/en/stable/getting-started-guide/installing.html\nhttps://docs.xarray.dev/en/stable/user-guide/io.html"
     ]
    }
   ],
   "source": [
    "wmo_list = [4903532, 1902637]\n",
    "#Float 1 = test float in the Icelandic Bassin\n",
    "float_1_url = 'https://data-argo.ifremer.fr/dac/aoml/4903532/4903532_Sprof.nc'\n",
    "#Float 2 = test float on Custard with glider next to it\n",
    "float_2_url = 'https://data-argo.ifremer.fr/dac/coriolis/1902637/1902637_Sprof.nc'\n",
    "\n",
    "#List the floats\n",
    "floats_url = [float_1_url, float_2_url]\n",
    "\n",
    "#Assign the local float directory\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "floats_dir =  os.path.join(parent_dir, 'Data/Floats')\n",
    "\n",
    "#Create floats filename\n",
    "floats_filenames = []\n",
    "for i in floats_url:\n",
    "    filename = floats_dir + '/' + i.rsplit('/', 1)[1]\n",
    "    floats_filenames.append(filename)\n",
    "\n",
    "position_df = pd.DataFrame({'PROF_NUM' : str(), 'LONGITUDE' : [], 'LATITUDE' : [], 'float' : int()})\n",
    "last_position_df = pd.DataFrame({'PROF_NUM' : str(), 'LONGITUDE' : [], 'LATITUDE' : [], 'float' : int()})\n",
    "\n",
    "for file, wmo in zip(floats_filenames, wmo_list):\n",
    "    dat = xr.open_dataset(file)\n",
    "    dat = dat.rename({'CYCLE_NUMBER':'PROF_NUM'}).swap_dims({'N_PROF':'PROF_NUM'})\n",
    "    temp_df = dat[['LONGITUDE', 'LATITUDE', 'JULD']].to_dataframe().reset_index()\n",
    "    temp_df['float'] = wmo\n",
    "    last_temp_df = temp_df[temp_df['JULD'] == max(temp_df['JULD'])]\n",
    "\n",
    "    last_position_df = pd.concat([last_position_df, last_temp_df], ignore_index=True)\n",
    "    position_df = pd.concat([position_df, temp_df], ignore_index=True)\n",
    "    dat.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "navis_position['JULD'] = pd.to_datetime(navis_position['JULD'])\n",
    "full_position = pd.concat([position_df, navis_position], ignore_index = True)\n",
    "\n",
    "most_recent_date = pd.to_datetime(full_position['JULD']).max().strftime('%Y_%m_%d')\n",
    "output_filename = f'Float_positions_{most_recent_date}.csv'\n",
    "# Save the dataframe to a CSV file\n",
    "full_position.to_csv(output_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.feature as cfeature\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "land_50m = cfeature.NaturalEarthFeature('physical', 'land', '50m',\n",
    "                                        edgecolor='k',\n",
    "                                        facecolor=cfeature.COLORS['land'])\n",
    "\n",
    "# Define data's extents I used an arbitrary extent that depicts the Icelandic Bassin\n",
    "min_lon = -35\n",
    "max_lon = -5\n",
    "min_lat = 55\n",
    "max_lat = 66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bath_directory = parent_dir + '/Data/' + \"ne_10m_bathymetry_all/\"\n",
    "def load_bathymetry(zip_file_url):\n",
    "    \"\"\"Read zip file from Natural Earth containing bathymetry shapefiles\"\"\"\n",
    "    # Download and extract shapefiles\n",
    "    import io\n",
    "    import zipfile\n",
    "\n",
    "    #r = requests.get(zip_file_url)\n",
    "    #z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    #z.extractall(bath_directory)\n",
    "\n",
    "    # Read shapefiles, sorted by depth\n",
    "    shp_dict = {}\n",
    "    files = glob(bath_directory + '*.shp')\n",
    "    assert len(files) > 0\n",
    "    files.sort()\n",
    "    depths = []\n",
    "    for f in files:\n",
    "        depth = '-' + f.split('_')[-1].split('.')[0]  # depth from file name\n",
    "        depths.append(depth)\n",
    "        bbox = (min_lon - 3, max_lon + 3,min_lat - 1, max_lat + 1)  # (x0, y0, x1, y1)\n",
    "        nei = shpreader.Reader(f, bbox=bbox)\n",
    "        shp_dict[depth] = nei\n",
    "    depths = np.array(depths)[::-1]  # sort from surface to bottom\n",
    "    return depths, shp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.io.shapereader as shpreader\n",
    "depths_str, shp_dict = load_bathymetry(\n",
    "        'https://naturalearth.s3.amazonaws.com/' +\n",
    "        '10m_physical/ne_10m_bathymetry_all.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from datetime import datetime\n",
    "grouped = full_position.groupby('float')\n",
    "\n",
    "# Construct a discrete colormap with colors corresponding to each depth\n",
    "depths = depths_str.astype(int)\n",
    "N = len(depths)\n",
    "nudge = 0.01  # shift bin edge slightly to include data\n",
    "boundaries = [min(depths)] + sorted(depths+nudge)  # low to high\n",
    "norm = matplotlib.colors.BoundaryNorm(boundaries, N)\n",
    "blues_cm = matplotlib.colormaps['Blues_r'].resampled(N)\n",
    "colors_depths = blues_cm(norm(depths))\n",
    "\n",
    "# Set up plot\n",
    "# Initialize an empty figure and add an axis\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(1, 1, 1,\n",
    "                    projection=ccrs.Mercator())\n",
    "\n",
    "# Set the map extent based on your latitude and longitude ranges\n",
    "ax.set_extent([min_lon, max_lon, min_lat, max_lat], crs=ccrs.PlateCarree())\n",
    "\n",
    "# Scatter plot\n",
    "sc = ax.scatter(full_position['LONGITUDE'], full_position['LATITUDE'], transform=ccrs.PlateCarree(), c = full_position['JULD'], zorder = 3)\n",
    "\n",
    "#set the plot color bar\n",
    "cbar = plt.colorbar(sc, ax = ax, label='Date')\n",
    "cbar.set_label('Date', rotation=270, labelpad=15)\n",
    "\n",
    "float_array = cbar.ax.get_yticks()\n",
    "formatted_date = np.vectorize(lambda x: datetime.fromtimestamp(float(x) / 1e9).strftime(\"%b %Y\"))(float_array)\n",
    "cbar.ax.set_yticklabels(formatted_date)\n",
    "\n",
    "for i, depth_str in enumerate(depths_str):\n",
    "    ax.add_geometries(shp_dict[depth_str].geometries(),\n",
    "                        crs=ccrs.PlateCarree(),\n",
    "                        color=colors_depths[i])\n",
    "\n",
    "for name, group in grouped:\n",
    "    group.plot(x='LONGITUDE', y='LATITUDE', ax=ax, transform=ccrs.PlateCarree(), label=name, zorder=2)\n",
    "# Add map features\n",
    "ax.add_feature(cfeature.COASTLINE)\n",
    "ax.add_feature(land_50m)\n",
    "ax.stock_img()\n",
    "\n",
    "# print a grid on it\n",
    "gl = ax.gridlines(draw_labels=True,x_inline=False,y_inline=False, crs=ccrs.PlateCarree())\n",
    "\n",
    "# Convert vector bathymetries to raster (saves a lot of disk space)\n",
    "# while leaving labels as vectors\n",
    "ax.set_rasterized(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nrt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
