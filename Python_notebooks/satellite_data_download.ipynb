{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Load python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think a lot of these are unnecessary!\n",
    "import os\n",
    "import copernicusmarine as cpm\n",
    "from pathlib import Path\n",
    "import ast\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create directories to store data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data' folder already exists in the parent directory.\n",
      "'satellite' directory already exists inside 'data' folder.\n"
     ]
    }
   ],
   "source": [
    "def create_missing_directories():\n",
    "    # Define the path to the parent directory\n",
    "    parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "\n",
    "    # Check if 'data' folder exists in the parent directory\n",
    "    data_dir = os.path.join(parent_dir, 'data')\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "        print(\"'data' folder created in the parent directory.\")\n",
    "    else:\n",
    "        print(\"'data' folder already exists in the parent directory.\")\n",
    "\n",
    "    # Check if 'satellite' directory exists inside 'data' folder\n",
    "    satellite_dir = os.path.join(data_dir, 'satellite')\n",
    "    if not os.path.exists(satellite_dir):\n",
    "        os.makedirs(satellite_dir)\n",
    "        print(\"'satellite' directory created inside 'data' folder.\")\n",
    "    else:\n",
    "        print(\"'satellite' directory already exists inside 'data' folder.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_missing_directories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it is the first time running copernicusmarine module on this machine you must login. </br>\n",
    "Using the terminal,</br>\n",
    "<pre>copernicusmarine login</pre>\n",
    "or,</br>\n",
    "<pre>cpm.login()</pre>\n",
    "and enter your credentials.</br>\n",
    "You will need an account to do this. (https://data.marine.copernicus.eu/register) </br>\n",
    "Your credentials are stored locally, you should only have to do this once per machine.\n",
    "\n",
    "\n",
    "\n",
    "When changing the catalogue search term, use the name of the product you'd like, e.g., from this domain, (https://data.marine.copernicus.eu/product/OCEANCOLOUR_ATL_BGC_L3_NRT_009_111/services), any of the named Dataset ID's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_to_extract = 'cmems_obs-oc_atl_bgc-plankton_nrt_l3-multi-1km_P1D' # for chla\n",
    "\n",
    "#dataset_to_extract = 'cmems_obs-oc_atl_bgc-optics_nrt_l3-multi-1km_P1D' # for bbp\n",
    "\n",
    "#dataset_to_extract = 'cmems_obs-sl_eur_phy-ssh_nrt_allsat-l4-duacs-0.125deg_P1D' #for current"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note directories you wish to store data in, set bounds for time, space, depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir    = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "data_dir      = os.path.join(parent_dir, 'data')\n",
    "satellite_dir = os.path.join(data_dir, 'satellite')\n",
    "\n",
    "#variables_to_extract = [var_to_extract] # Input for variable must be list\n",
    "\n",
    "start_time = \"2024-05-01T00:00:00\" # Format time as YYYY-MM-DD\"T\"HH:MM:SS ,without T in parenthesis\n",
    "\n",
    "# Get the current date and time\n",
    "now = datetime.now()\n",
    "# Format the date and time\n",
    "formatted_date = now.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "#set it as the end time\n",
    "end_time   = formatted_date\n",
    "\n",
    "min_depth = 0 # Primarily interested in surface values only. Will flag if outside of range of product\n",
    "max_depth = 1\n",
    "\n",
    "min_lon = -35\n",
    "max_lon = -5\n",
    "min_lat = 55\n",
    "max_lat = 66\n",
    "\n",
    "saved_netCDF_as = \"CMEMS_Iceland_Basin_CHLA_April-May2024.nc\" # This changes the name of the output file.\n",
    "\n",
    "filepath = os.path.join(satellite_dir, saved_netCDF_as)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMEMS_Iceland_Basin_CHLA_April-May2024.nc does not exist. Proceeding to download.\n",
      "INFO - 2024-05-25T09:51:08Z - Dataset version was not specified, the latest one was selected: \"202311\"\n",
      "INFO - 2024-05-25T09:51:08Z - Dataset part was not specified, the first one was selected: \"default\"\n",
      "INFO - 2024-05-25T09:51:15Z - Service was not specified, the default one was selected: \"arco-geo-series\"\n",
      "WARNING - 2024-05-25T09:51:21Z - Some or all of your subset selection [55, 66] for the latitude dimension  exceed the dataset coordinates [20.005207061767578, 65.99478912353516]\n",
      "WARNING - 2024-05-25T09:51:21Z - Some or all of your subset selection [2024-05-01 00:00:00, 2024-05-25 10:50:17] for the time dimension  exceed the dataset coordinates [2024-05-07 23:00:00, 2024-05-23 00:00:00]\n",
      "INFO - 2024-05-25T09:51:21Z - Downloading using service arco-geo-series...\n",
      "INFO - 2024-05-25T09:51:27Z - <xarray.Dataset> Size: 6GB\n",
      "Dimensions:              (time: 16, latitude: 1056, longitude: 2880)\n",
      "Coordinates:\n",
      "  * latitude             (latitude) float32 4kB 55.01 55.02 ... 65.98 65.99\n",
      "  * longitude            (longitude) float32 12kB -34.99 -34.98 ... -5.005\n",
      "  * time                 (time) datetime64[ns] 128B 2024-05-08 ... 2024-05-23\n",
      "Data variables: (12/21)\n",
      "    CHL                  (time, latitude, longitude) float32 195MB dask.array<chunksize=(1, 1024, 512), meta=np.ndarray>\n",
      "    CHL_uncertainty      (time, latitude, longitude) float64 389MB dask.array<chunksize=(1, 1024, 512), meta=np.ndarray>\n",
      "    DIATO                (time, latitude, longitude) float32 195MB dask.array<chunksize=(1, 1024, 512), meta=np.ndarray>\n",
      "    DIATO_uncertainty    (time, latitude, longitude) float64 389MB dask.array<chunksize=(1, 1024, 512), meta=np.ndarray>\n",
      "    DINO                 (time, latitude, longitude) float32 195MB dask.array<chunksize=(1, 1024, 512), meta=np.ndarray>\n",
      "    DINO_uncertainty     (time, latitude, longitude) float64 389MB dask.array<chunksize=(1, 1024, 512), meta=np.ndarray>\n",
      "    ...                   ...\n",
      "    PICO_uncertainty     (time, latitude, longitude) float64 389MB dask.array<chunksize=(1, 1024, 512), meta=np.ndarray>\n",
      "    PROCHLO              (time, latitude, longitude) float32 195MB dask.array<chunksize=(1, 1024, 512), meta=np.ndarray>\n",
      "    PROCHLO_uncertainty  (time, latitude, longitude) float64 389MB dask.array<chunksize=(1, 1024, 512), meta=np.ndarray>\n",
      "    PROKAR               (time, latitude, longitude) float32 195MB dask.array<chunksize=(1, 1024, 512), meta=np.ndarray>\n",
      "    PROKAR_uncertainty   (time, latitude, longitude) float64 389MB dask.array<chunksize=(1, 1024, 512), meta=np.ndarray>\n",
      "    flags                (time, latitude, longitude) int8 49MB dask.array<chunksize=(1, 1024, 512), meta=np.ndarray>\n",
      "Attributes: (12/92)\n",
      "    Conventions:                     CF-1.8, ACDD-1.3\n",
      "    DPM_reference:                   GC-UD-ACRI-PUG\n",
      "    IODD_reference:                  GC-UD-ACRI-PUG\n",
      "    acknowledgement:                 The Licensees will ensure that original ...\n",
      "    citation:                        The Licensees will ensure that original ...\n",
      "    cmems_product_id:                OCEANCOLOUR_ATL_BGC_L3_NRT_009_111\n",
      "    ...                              ...\n",
      "    time_coverage_resolution:        P1D\n",
      "    time_coverage_start:             2023-10-15T06:57:27Z\n",
      "    title:                           cmems_obs-oc_atl_bgc-plankton_nrt_l3-mul...\n",
      "    westernmost_longitude:           -46.0\n",
      "    westernmost_valid_longitude:     -46.0\n",
      "    copernicusmarine_version:        1.2.2\n",
      "INFO - 2024-05-25T09:51:27Z - Estimated size of the dataset file is 3900.267 MB.\n",
      "Do you want to proceed with download? [Y/n]:INFO - 2024-05-25T09:51:35Z - Writing to local storage. Please wait...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30402/30402 [01:26<00:00, 351.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - 2024-05-25T09:53:05Z - Successfully downloaded to c:\\Users\\hanshil\\Documents\\GitHub\\biocarbon_nrt_data_viz\\data\\satellite\\CMEMS_Iceland_Basin_CHLA_April-May2024.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(filepath):\n",
    "  response = input(\"Do you want to remove and overwrite the file \" + f\"\\\"{saved_netCDF_as}\\\"\" + \" (y/n): \").strip().lower()\n",
    "  if response == 'y':\n",
    "        os.remove(filepath)\n",
    "        print(f\"{filepath} has been removed.\")\n",
    "        print(f\"{saved_netCDF_as}. Proceeding to download.\")\n",
    "        cpm.subset(dataset_id=dataset_to_extract,\n",
    "        minimum_longitude= min_lon,\n",
    "        maximum_longitude= max_lon,\n",
    "        minimum_latitude= min_lat,\n",
    "        maximum_latitude= max_lat,\n",
    "        start_datetime=start_time,\n",
    "        end_datetime=end_time,\n",
    "        minimum_depth=min_depth,\n",
    "        maximum_depth=max_depth,\n",
    "        output_filename = saved_netCDF_as,\n",
    "        output_directory = satellite_dir)\n",
    "        \n",
    "  else:\n",
    "        print(\"Download cancelled.\")\n",
    "else:\n",
    "  print(f\"{saved_netCDF_as} does not exist. Proceeding to download.\")\n",
    "  cpm.subset(dataset_id=dataset_to_extract,\n",
    "  minimum_longitude= min_lon,\n",
    "  maximum_longitude= max_lon,\n",
    "  minimum_latitude= min_lat,\n",
    "  maximum_latitude= max_lat,\n",
    "  start_datetime=start_time,\n",
    "  end_datetime=end_time,\n",
    "  minimum_depth=min_depth,\n",
    "  maximum_depth=max_depth,\n",
    "  output_filename = saved_netCDF_as,\n",
    "  output_directory = satellite_dir)\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
